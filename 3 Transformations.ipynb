{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from helpers import *\n",
    "import sys, PIL, matplotlib.pyplot as plt, itertools, math, random, collections, torch\n",
    "import scipy.stats, scipy.special\n",
    "\n",
    "import requests\n",
    "import pickle, gzip\n",
    "\n",
    "from enum import Enum, IntEnum\n",
    "from torch import tensor, Tensor, FloatTensor, LongTensor, ByteTensor, DoubleTensor, HalfTensor, ShortTensor\n",
    "from operator import itemgetter, attrgetter\n",
    "from numpy import cos, sin, tan, tanh, log, exp\n",
    "from dataclasses import field\n",
    "\n",
    "from functools import reduce\n",
    "from collections import defaultdict, abc, namedtuple, Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = Path('data')\n",
    "PATH = DATA_PATH/'mnist'\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL='http://deeplearning.net/data/mnist/'\n",
    "FILENAME='mnist.pkl.gz'\n",
    "\n",
    "if not (PATH/FILENAME).exists():\n",
    "    content = requests.get(URL+FILENAME).content\n",
    "    (PATH/FILENAME).open('wb').write(content)\n",
    "\n",
    "with gzip.open(PATH / FILENAME, 'rb') as f:\n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f,\n",
    "                                                              encoding='latin-1')\n",
    "\n",
    "\n",
    "def display_image(path='images/mnist_cnn.jpg'):\n",
    "    img = PIL.Image.open(path)\n",
    "    rgb_im = img.convert('RGB')\n",
    "    return rgb_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(0.9961))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train,y_train,x_valid,y_valid = map(torch.tensor, (x_train,y_train,x_valid,y_valid))\n",
    "x_train.min(),x_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bs=64\n",
    "epochs = 2\n",
    "lr=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "def mnist2image(b):\n",
    "    return b.view(1,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = DataBunch(train_ds, valid_ds, bs, train_tfm=mnist2image, valid_tfm=mnist2image)\n",
    "model = simple_cnn([1,16,16,10], [3,3,3], [2,2,2])\n",
    "learner = Learner(data, model)\n",
    "opt_fn = partial(optim.SGD, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = opt_fn(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b87ac732f14173840577fa84c470fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aebdc10e25a9492e95328ab0f54c464e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb7deff863dd4fc58796b66a756b1259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.513219372416\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a561bca17124dc4bd37c41ddc464bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dad64bc93c8b4268bbacd341c63e42b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.334691764545\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc10d785ff3b4efcbb2c760e192d8032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd5dfadb1a844ccf81db1768bb440e3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.254847696018\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa4d6e396f984e428d98be927b98d10b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f7ee8b1bc149f6a6ffc75f5c544921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.20911056242\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner.fit(4, lr/5, opt_fn=opt_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OptimWrapper():\n",
    "    def __init__(self, opt, wd=0, true_wd=False):\n",
    "        self.opt = opt\n",
    "        self.true_wd=true_wd\n",
    "        \n",
    "        #dict_keys(['params', 'lr', 'momentum', 'dampening', 'weight_decay', 'nesterov'])\n",
    "        self.opt_keys = list(self.opt.param_groups[0].keys())\n",
    "        self.opt_keys.remove('params')\n",
    "        self.read_defaults()\n",
    "        self._wd = wd\n",
    "        \n",
    "        \n",
    "    def read_defaults(self):\n",
    "        #basccilty setting up additonal default paramaters\n",
    "        self._beta = None\n",
    "        if 'lr' in self.opt_keys: \n",
    "            self._lr = self.opt.param_groups[0]['lr']\n",
    "        if 'momentum' in self.opt_keys:\n",
    "            self._mom = self.opt.param_groups[0]['momentum']\n",
    "        if 'alpha' in self.opt_keys: \n",
    "            self._beta = self.opt.param_groups[0]['alpha']\n",
    "        if 'betas' in self.opt_keys: \n",
    "            self._mom,self._beta = self.opt.param_groups[0]['betas']\n",
    "        if 'weight_decay' in self.opt_keys: \n",
    "            self._wd = self.opt.param_groups[0]['weight_decay']\n",
    "        \n",
    "    def step(self):\n",
    "        if self.true_wd:\n",
    "            #here bassicly we are goung to paramater groups\n",
    "            for pg in self.opt.param_groups:\n",
    "                #accessing all the weightds and applying weight decays\n",
    "                for p in pg['params']: \n",
    "                    p.data.mul_(1 - self._wd*pg['lr'])\n",
    "                #setting validdation weight decay to 0\n",
    "                self.set_val('weight_decay', 0)\n",
    "        self.opt.step()\n",
    "\n",
    "    def set_val(self, key, val):\n",
    "        for pg in self.opt.param_groups: \n",
    "            pg[key] = val\n",
    "        return val\n",
    "                \n",
    "    def zero_grad(self): \n",
    "        self.opt.zero_grad()\n",
    "        \n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "    \n",
    "    @lr.setter\n",
    "    def lr(self, val): \n",
    "        self._lr = self.set_val('lr', val)\n",
    "        \n",
    "    @property\n",
    "    def mom(self): \n",
    "        return self._mom\n",
    "    \n",
    "    @mom.setter\n",
    "    def mom(self, val):\n",
    "        if 'momentum' in self.opt_keys: \n",
    "            self.set_val('momentum', val)\n",
    "        elif 'betas' in self.opt_keys:  \n",
    "            self.set_val('betas', (val, self._beta))\n",
    "        self._mom = val\n",
    "        \n",
    "    @property\n",
    "    def beta(self): \n",
    "        return self._beta\n",
    "\n",
    "    @beta.setter\n",
    "    def beta(self, val):\n",
    "        if 'betas' in self.opt_keys:    \n",
    "            self.set_val('betas', (self._mom,val))\n",
    "        elif 'alpha' in self.opt_keys:  \n",
    "            self.set_val('alpha', val)\n",
    "        self._beta = val\n",
    "        \n",
    "    @property\n",
    "    def wd(self): return self._wd\n",
    "\n",
    "    @wd.setter\n",
    "    def wd(self, val):\n",
    "        if not self.true_wd: \n",
    "            self.set_val('weight_decay', val)\n",
    "        self._wd = val\n",
    "        \n",
    "    def set_val(self, key, val):\n",
    "        for pg in self.opt.param_groups: \n",
    "            pg[key] = val\n",
    "        return val\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.01, 0.95, 0, 0.99)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_fn = partial(optim.Adam, betas=(0.95,0.99))\n",
    "opt = OptimWrapper(opt_fn(model.parameters(), 1e-2))\n",
    "opt.lr, opt.mom, opt.wd, opt.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class Callback():\n",
    "    def on_train_begin(self, **kwargs):\n",
    "        \n",
    "        pass         \n",
    "        #To initiliaze constants in the callback.\n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        pass\n",
    "        #At the beginning of each epoch\n",
    "    def on_batch_begin(self, **kwargs):\n",
    "        pass \n",
    "        #To set HP before the step is done. \n",
    "        #Returns xb, yb (which can allow us to modify the input at that step if needed)\n",
    "    def on_loss_begin(self, **kwargs):\n",
    "        pass\n",
    "        #Called after the forward pass but before the loss has been computed.\n",
    "        #Returns the output (which can allow us to modify it)\n",
    "    def on_backward_begin(self, **kwargs): \n",
    "        pass\n",
    "        #Called after the forward pass and the loss has been computed, but before the back propagation.\n",
    "        #Returns the loss (which can allow us to modify it, for instance for reg functions)\n",
    "    def on_backward_end(self, **kwargs): \n",
    "        pass\n",
    "        #Called after the back propagation had been done (and the gradients computed) but before the step of the optimizer.\n",
    "        #Useful for true weight decay in AdamW\n",
    "    def on_step_end(self, **kwargs): \n",
    "        pass\n",
    "        #Called after the step of the optimizer but before the gradients are zeroed (not sure this one is useful)\n",
    "    def on_batch_end(self, **kwargs): \n",
    "        pass\n",
    "        #Called at the end of the batch\n",
    "    def on_epoch_end(self, **kwargs):\n",
    "        pass\n",
    "        #Called at the end of an epoch\n",
    "    def on_train_end(self, **kwargs):\n",
    "        pass\n",
    "        #Useful for cleaning up things and saving files/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class SmoothenValue():\n",
    "    def __init__(self, beta):\n",
    "        self.beta = beta\n",
    "        self.n = 0\n",
    "        self.mov_avg = 0\n",
    "    \n",
    "    def add_value(self, val):\n",
    "        self.n += 1\n",
    "        self.mov_avg = self.beta * self.mov_avg + (1 - self.beta) * val\n",
    "        self.smooth = self.mov_avg / (1 - self.beta ** self.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#export\n",
    "def _get_init_state(): \n",
    "    return {'epoch':0, 'iteration':0, 'num_batch':0}\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CallbackHandler():\n",
    "    callbacks:Collection[Callable]\n",
    "    beta:float=0.98\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        self.smoothener = SmoothenValue(self.beta)\n",
    "        self.state_dict:Dict[str,Union[int,float,Tensor]]=_get_init_state()\n",
    "    \n",
    "    def __call__(self, cb_name):\n",
    "        return [getattr(cb, f'on_{cb_name}')(**self.state_dict) for cb in self.callbacks]\n",
    "    \n",
    "    def on_train_begin(self): \n",
    "        self.state_dict = _get_init_state()\n",
    "        self('train_begin')\n",
    "        \n",
    "    def on_epoch_begin(self): \n",
    "        self.state_dict['num_batch'] = 0\n",
    "        self('epoch_begin')\n",
    "        \n",
    "    def on_batch_begin(self, xb, yb):\n",
    "        self.state_dict['last_input'] = xb\n",
    "        self.state_dict['last_target'] = yb\n",
    "        for cb in self.callbacks:\n",
    "            a = cb.on_batch_begin(**self.state_dict)\n",
    "            if a is not None: \n",
    "                self.state_dict['last_input'], self.state_dict['last_target'] = a\n",
    "        return self.state_dict['last_input'], self.state_dict['last_target']\n",
    "    \n",
    "    def on_loss_begin(self, out):\n",
    "        self.state_dict['last_output'] = out\n",
    "        for cb in self.callbacks:\n",
    "            a = cb.on_loss_begin(**self.state_dict)\n",
    "            if a is not None: \n",
    "                self.state_dict['last_output'] = a\n",
    "                \n",
    "\n",
    "        return self.state_dict['last_output']\n",
    "    \n",
    "    def on_backward_begin(self, loss):\n",
    "        self.smoothener.add_value(loss.item())\n",
    "        self.state_dict['last_loss'], self.state_dict['smooth_loss'] = loss, self.smoothener.smooth\n",
    "        for cb in self.callbacks:\n",
    "            a = cb.on_backward_begin(**self.state_dict)\n",
    "            if a is not None:\n",
    "                self.state_dict['last_loss'] = a\n",
    "        return self.state_dict['last_loss']\n",
    "    \n",
    "    def on_backward_end(self):        \n",
    "        self('backward_end')\n",
    "    \n",
    "    def on_step_end(self):           \n",
    "        self('step_end')\n",
    "        \n",
    "    def on_batch_end(self, loss):     \n",
    "        self.state_dict['last_loss'] = loss\n",
    "        stop = np.any(self('batch_end'))\n",
    "        self.state_dict['iteration'] += 1\n",
    "        self.state_dict['num_batch'] += 1\n",
    "        return stop\n",
    "    \n",
    "    def on_epoch_end(self, val_metrics):\n",
    "        self.state_dict['last_metrics'] = val_metrics\n",
    "        stop = np.any(self('epoch_end'))\n",
    "        self.state_dict['epoch'] += 1\n",
    "        return stop\n",
    "    \n",
    "    def on_train_end(self): \n",
    "        self('train_end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def loss_batch(model, xb, yb, loss_fn, opt=None, cb_handler=None, metrics=None):\n",
    "    if cb_handler is None: \n",
    "        cb_handler = CallbackHandler([])\n",
    "    out = model(xb)\n",
    "    #getting out shapee\n",
    "    out = cb_handler.on_loss_begin(out)\n",
    " \n",
    "    #getting losss\n",
    "    loss = loss_fn(out, yb)\n",
    "    \n",
    "    #caclulating custom metrics\n",
    "    mets = [f(out,yb).item() for f in metrics] if metrics is not None else []\n",
    "    \n",
    "    if opt is not None:\n",
    "        loss = cb_handler.on_backward_begin(loss)\n",
    "        loss.backward()\n",
    "        cb_handler.on_backward_end()\n",
    "        opt.step()\n",
    "        cb_handler.on_step_end()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    return (loss.item(),) + tuple(mets) + (len(xb),)\n",
    "\n",
    "\n",
    "def fit(epochs, model, loss_fn, opt, data, callbacks=None, metrics=None):\n",
    "    cb_handler = CallbackHandler(callbacks)\n",
    "    cb_handler.on_train_begin()\n",
    "    \n",
    "    for epoch in tnrange(epochs):\n",
    "        model.train()\n",
    "        cb_handler.on_epoch_begin()\n",
    "  \n",
    "        for xb,yb in data.train_dl:\n",
    "            xb, yb = cb_handler.on_batch_begin(xb, yb)\n",
    "            loss,_ = loss_batch(model, xb, yb, loss_fn, opt, cb_handler)\n",
    "            if cb_handler.on_batch_end(loss):\n",
    "                break\n",
    "        \n",
    "        if hasattr(data,'valid_dl') and data.valid_dl is not None:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                *val_metrics,nums = zip(*[loss_batch(model, xb, yb, loss_fn, cb_handler=cb_handler, metrics=metrics)\n",
    "                                for xb,yb in data.valid_dl])\n",
    "            val_metrics = [np.sum(np.multiply(val,nums)) / np.sum(nums) for val in val_metrics]\n",
    "            \n",
    "        else: \n",
    "            val_metrics=None\n",
    "        if cb_handler.on_epoch_end(val_metrics):\n",
    "            break\n",
    "        \n",
    "    cb_handler.on_train_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Recorder(Callback):\n",
    "    opt: torch.optim\n",
    "    train_dl: DeviceDataLoader = None\n",
    "    \n",
    "    def on_train_begin(self, **kwargs):\n",
    "        self.losses,self.val_losses,self.metrics,self.lrs,self.moms = [],[],[],[],[]\n",
    "    \n",
    "    def on_batch_begin(self, **kwargs):\n",
    "        self.lrs.append(self.opt.lr)\n",
    "        self.moms.append(self.opt.mom)\n",
    "    \n",
    "    def on_batch_end(self, last_loss, **kwargs):\n",
    "        self.losses.append(last_loss)\n",
    "        if self.train_dl is not None and self.train_dl.progress_func is not None: \n",
    "            self.train_dl.gen.set_postfix_str(last_loss)\n",
    "    \n",
    "    def on_epoch_end(self, epoch, last_loss, last_metrics, **kwargs):\n",
    "        if last_metrics is not None:\n",
    "            self.val_losses.append(last_metrics[0])\n",
    "            if len(last_metrics) > 1: self.metrics.append(last_metrics[1:])\n",
    "            print(epoch, last_loss, *last_metrics)\n",
    "        else:  print(epoch, last_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#export\n",
    "def accuracy(out, yb):\n",
    "    preds = torch.max(out, dim=1)[1]\n",
    "    return (preds==yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, data, model):\n",
    "        self.data,self.model = data,model.to(data.device)\n",
    "        self.loss_fn, self.opt_fn, self.metrics = F.cross_entropy, optim.SGD, [accuracy]\n",
    "        \n",
    "    @property\n",
    "    def train_dl(self): \n",
    "        return self.data.train_dl\n",
    "    @property\n",
    "    def valid_dl(self): \n",
    "        return self.data.train_dl if hasattr(self.data, 'valid_dl') else None\n",
    "\n",
    "    def fit(self, epochs, lr, callbacks=None):\n",
    "        if callbacks is None: \n",
    "            callbacks=[]\n",
    "        self.opt = OptimWrapper(self.opt_fn(self.model.parameters(),lr))\n",
    "        self.recorder = Recorder(self.opt, self.train_dl)\n",
    "        callbacks = [self.recorder] + callbacks\n",
    "        fit(epochs, self.model, self.loss_fn, self.opt, self.data, callbacks=callbacks, metrics=self.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = DataBunch(train_ds, valid_ds, bs, train_tfm=mnist2image, valid_tfm=mnist2image)\n",
    "model = simple_cnn([1,16,16,10], [3,3,3], [2,2,2])\n",
    "learn = Learner(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(learn.recorder.val_losses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def annealing_no(start, end, pct): \n",
    "    return start\n",
    "\n",
    "def annealing_linear(start, end, pct): \n",
    "    return start + pct * (end-start)\n",
    "\n",
    "def annealing_exp(start, end, pct): \n",
    "    return start * (end/start) ** pct\n",
    "\n",
    "def annealing_cos(start, end, pct):\n",
    "    cos_out = np.cos(np.pi * pct) + 1\n",
    "    return end + (start-end)/2 * cos_out\n",
    "    \n",
    "def do_annealing_poly(start, end, pct, degree): \n",
    "    return end + (start-end) * (1-pct)**degree\n",
    "\n",
    "def annealing_poly(degree):\n",
    "    return functools.partial(do_annealing_poly, degree=degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "annealings = \"NO LINEAR COS EXP POLY\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.arange(0, 100)\n",
    "p = np.linspace(0.01,1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fns = [annealing_no, annealing_linear, annealing_cos, annealing_exp, annealing_poly(0.8)]\n",
    "for fn, t in zip(fns, annealings):\n",
    "    plt.plot(a, [fn(2, 1e-2, o) for o in p], label=t)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_tuple(x): \n",
    "    return isinstance(x, tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Stepper():\n",
    "    def __init__(self, vals, num_it, ft=None):\n",
    "        self.start, self.end = (vals[0],vals[1]) if is_tuple(vals) else (vals,0)\n",
    "        self.num_it = num_it\n",
    "        if ft is None: self.ft = annealing_linear if is_tuple(vals) else annealing_no\n",
    "        else:          \n",
    "            self.ft = ft\n",
    "        self.n = 0\n",
    "    \n",
    "    def step(self):\n",
    "        self.n += 1\n",
    "        return self.ft(self.start, self.end, self.n/self.num_it)\n",
    "    \n",
    "    @property\n",
    "    def is_done(self):  \n",
    "        return self.n >= self.num_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class OneCycleScheduler(Callback):\n",
    "    def __init__(self, learn, lr_max, epochs, moms=(0.95,0.85), div_factor=10, pct_end=0.1):\n",
    "        self.learn = learn\n",
    "        a = int(len(learn.data.train_dl) * epochs * (1 - pct_end) / 2)\n",
    "        b = len(learn.data.train_dl) * epochs - 2*a\n",
    "        self.lr_scheds = [Stepper((lr_max/div_factor, lr_max), a),\n",
    "                          Stepper((lr_max, lr_max/div_factor), a),\n",
    "                          Stepper((lr_max/div_factor, lr_max/(div_factor*100)), b)]\n",
    "        self.mom_scheds = [Stepper(moms, a), Stepper((moms[1], moms[0]), a), Stepper(moms[0], b)]\n",
    "    \n",
    "    def on_train_begin(self, **kwargs):\n",
    "        self.opt = self.learn.opt\n",
    "        self.opt.lr, self.opt.mom = self.lr_scheds[0].start, self.mom_scheds[0].start\n",
    "        self.idx_s = 0\n",
    "    \n",
    "    def on_batch_end(self, **kwargs):\n",
    "        if self.idx_s >= len(self.lr_scheds): \n",
    "            return True\n",
    "        self.opt.lr = self.lr_scheds[self.idx_s].step()\n",
    "        self.opt.mom = self.mom_scheds[self.idx_s].step()\n",
    "        if self.lr_scheds[self.idx_s].is_done:\n",
    "            self.idx_s += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = DataBunch(train_ds, valid_ds, bs, train_tfm=mnist2image, valid_tfm=mnist2image)\n",
    "model = simple_cnn([1,16,16,10], [3,3,3], [2,2,2])\n",
    "learn = Learner(data, model)\n",
    "sched = OneCycleScheduler(learn, 0.1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.fit(3,0.1,callbacks=[sched])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterations = list(range(len(learn.recorder.lrs)))\n",
    "fig, axs = plt.subplots(1,2, figsize=(12,4))\n",
    "axs[0].plot(iterations, learn.recorder.lrs)\n",
    "axs[1].plot(iterations, learn.recorder.moms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "@dataclass\n",
    "class Learner():\n",
    "    data: DataBunch\n",
    "    model: nn.Module\n",
    "    opt_fn: Callable = optim.SGD\n",
    "#     loss_fn: LossType = F.cross_entropy\n",
    "    loss_fn: Callable = F.cross_entropy\n",
    "    metrics: Collection[Callable] = None\n",
    "    true_wd: bool = False\n",
    "    def __post_init__(self): self.model = self.model.to(self.data.device)\n",
    "\n",
    "    def fit(self, epochs, lr, wd=0., callbacks=None):\n",
    "        if not hasattr(self, 'opt'): self.create_opt(lr, wd)\n",
    "        self.recorder = Recorder(self.opt, self.data.train_dl)\n",
    "        if callbacks is None: callbacks = []\n",
    "        callbacks = [self.recorder]+callbacks\n",
    "        fit(epochs, self.model, self.loss_fn, self.opt, self.data, callbacks=callbacks, metrics=self.metrics)\n",
    "    \n",
    "    def create_opt(self, lr, wd=0.):\n",
    "        self.opt = OptimWrapper(self.opt_fn(self.model.parameters(), lr), wd=wd, true_wd=self.true_wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class LRFinder(Callback):\n",
    "    def __init__(self, opt, data, start_lr=1e-5, end_lr=10, num_it=200):\n",
    "        self.opt,self.data = opt,data\n",
    "        self.sched = Stepper((start_lr, end_lr), num_it, annealing_exp)\n",
    "        #To avoid validating if the train_dl has less than num_it batches, we put aside the valid_dl and remove it\n",
    "        #during the call to fit.\n",
    "        self.valid_dl = data.valid_dl\n",
    "        self.data.valid_dl = None\n",
    "    \n",
    "    def on_train_begin(self, **kwargs):\n",
    "        self.opt.lr = self.sched.start\n",
    "        self.stop,self.best_loss = False,0.\n",
    "    \n",
    "    def on_batch_end(self, iteration, smooth_loss, **kwargs):\n",
    "        if iteration==0 or smooth_loss < self.best_loss: self.best_loss = smooth_loss\n",
    "        self.opt.lr = self.sched.step()\n",
    "        if self.sched.is_done or smooth_loss > 4*self.best_loss:\n",
    "            #We use the smoothed loss to decide on the stopping since it's less shaky.\n",
    "            self.stop=True\n",
    "            return True\n",
    "    \n",
    "    def on_epoch_end(self, **kwargs): return self.stop\n",
    "    \n",
    "    def on_train_end(self, **kwargs):\n",
    "        #Clean up and put back the valid_dl in its place.\n",
    "        self.data.valid_dl = self.valid_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lr_find(learn, start_lr=1e-5, end_lr=10, num_it=100):\n",
    "    #TODO: add model.save and model.load.\n",
    "    learn.create_opt(start_lr)\n",
    "    cb = LRFinder(learn.opt, learn.data, start_lr, end_lr, num_it)\n",
    "    a = int(np.ceil(num_it/len(learn.data.train_dl)))\n",
    "    learn.fit(a, start_lr, callbacks=[cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Recorder(Callback):\n",
    "    opt: torch.optim\n",
    "    train_dl: DeviceDataLoader = None\n",
    "\n",
    "    def on_train_begin(self, **kwargs):\n",
    "        self.losses,self.val_losses,self.lrs,self.moms,self.metrics,self.nb_batches = [],[],[],[],[],[]\n",
    "    \n",
    "    def on_batch_begin(self, **kwargs):\n",
    "        self.lrs.append(self.opt.lr)\n",
    "        self.moms.append(self.opt.mom)\n",
    "    \n",
    "    def on_backward_begin(self, smooth_loss, **kwargs):\n",
    "        #We record the loss here before any other callback has a chance to modify it.\n",
    "        self.losses.append(smooth_loss)\n",
    "        if self.train_dl is not None and self.train_dl.progress_func is not None: \n",
    "            self.train_dl.gen.set_postfix_str(smooth_loss)\n",
    "    \n",
    "    def on_epoch_end(self, epoch, num_batch, smooth_loss, last_metrics, **kwargs):\n",
    "        self.nb_batches.append(num_batch)\n",
    "        if last_metrics is not None:\n",
    "            self.val_losses.append(last_metrics[0])\n",
    "            if len(last_metrics) > 1: self.metrics.append(last_metrics[1:])\n",
    "            print(epoch, smooth_loss, *last_metrics)\n",
    "        else:  print(epoch, smooth_loss)\n",
    "    \n",
    "    def plot_lr(self, show_moms=False):\n",
    "        iterations = list(range(len(self.lrs)))\n",
    "        if show_moms:\n",
    "            _, axs = plt.subplots(1,2, figsize=(12,4))\n",
    "            axs[0].plot(iterations, self.lrs)\n",
    "            axs[1].plot(iterations, self.moms)\n",
    "        else: plt.plot(iterations, self.lrs)\n",
    "    \n",
    "    def plot(self, skip_start=10, skip_end=5):\n",
    "        lrs = self.lrs[skip_start:-skip_end] if skip_end > 0 else self.lrs[skip_start:]\n",
    "        losses = self.losses[skip_start:-skip_end] if skip_end > 0 else self.losses[skip_start:]\n",
    "        _, ax = plt.subplots(1,1)\n",
    "        ax.plot(lrs, losses)\n",
    "        ax.set_xscale('log')\n",
    "    \n",
    "    def plot_losses(self):\n",
    "        _, ax = plt.subplots(1,1)\n",
    "        iterations = list(range(len(self.losses)))\n",
    "        ax.plot(iterations, self.losses)\n",
    "        val_iter = self.nb_batches\n",
    "        val_iter = np.cumsum(val_iter)\n",
    "        ax.plot(val_iter, self.val_losses)\n",
    "    \n",
    "    def plot_metrics(self):\n",
    "        assert len(self.metrics) != 0, \"There is no metrics to plot.\"\n",
    "        _, axes = plt.subplots(len(self.metrics[0]),1,figsize=(6, 4*len(self.metrics[0])))\n",
    "        val_iter = self.nb_batches\n",
    "        val_iter = np.cumsum(val_iter)\n",
    "        axes = axes.flatten() if len(self.metrics[0]) != 1 else [axes]\n",
    "        for i, ax in enumerate(axes):\n",
    "            values = [met[i] for met in self.metrics]\n",
    "            ax.plot(val_iter, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = DataBunch(train_ds, valid_ds, bs, train_tfm=mnist2image, valid_tfm=mnist2image)\n",
    "model = simple_cnn([1,16,16,10], [3,3,3], [2,2,2])\n",
    "learn = Learner(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_find(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = DataBunch(train_ds, valid_ds, bs, train_tfm=mnist2image, valid_tfm=mnist2image)\n",
    "model = simple_cnn([1,16,16,10], [3,3,3], [2,2,2])\n",
    "learn = Learner(data, model)\n",
    "sched = OneCycleScheduler(learn, 0.0001, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.fit(5, 5e-3, callbacks=[sched])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
